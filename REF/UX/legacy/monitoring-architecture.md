# AI Cost Monitoring Agent â€” Monitoring & Observability Architecture

## Context

This document defines the monitoring, observability, and cost protection architecture for the **AI Cost Monitoring product** â€” a conversational AI agent that helps SMBs optimize their GCP cloud spend. The monitoring stack serves two purposes:

1. **Operational health** â€” Ensuring the product itself runs reliably for customers
2. **LLM cost control** â€” Protecting product margins by monitoring and limiting the AI consumption that powers the agent

The product's unit economics depend directly on controlling LLM costs per customer session. With a three-tier pricing model ($49/$99/$199/mo), every dollar of uncontrolled LLM spend erodes margin. A single runaway agent session â€” recursive tool calls, oversized context windows, or unbounded reasoning loops â€” can destroy profitability on a customer for the entire billing period.

---

## Architecture Overview

The monitoring architecture consists of **three layers** that work together as a unified observability stack, all deployable via Docker Compose or Cloud Run.

```
Layer 1: AI Gateway        â€” LiteLLM Proxy (cost enforcement, routing, rate limiting)
Layer 2: LLM Observability â€” Langfuse (traces, cost attribution, session analysis)
Layer 3: Infrastructure    â€” Grafana + Prometheus + Loki (compute, networking, logs)
```

All three layers feed into **Grafana** as the single pane of glass for operational visibility.

---

## Layer 1: AI Gateway â€” LiteLLM Proxy

### Role

LiteLLM acts as the **mandatory gateway** between the MCP Server agent and all LLM providers. Every LLM call â€” whether to Anthropic Claude, Google Gemini, or a self-hosted model â€” routes through LiteLLM. This is the enforcement layer that prevents cost blowouts.

### Capabilities

| Capability | Purpose | Priority |
|---|---|---|
| **Budget Caps** | Hard per-customer and per-session spend limits | CRITICAL |
| **Rate Limiting** | Tokens-per-minute caps to catch runaway loops | CRITICAL |
| **Cost Tracking** | Real-time token counting and cost calculation per request | HIGH |
| **Model Routing** | Route simple queries to cheaper models (Haiku/Flash), complex to Sonnet/Pro | HIGH |
| **Caching** | Cache repeated prompt patterns to reduce redundant API calls | MEDIUM |
| **Fallback/Retry** | Automatic failover between providers on errors or rate limits | MEDIUM |
| **Unified API** | Single OpenAI-compatible endpoint regardless of downstream provider | HIGH |

### Budget Enforcement Strategy

```
Per-Customer Limits (Monthly):
â”œâ”€â”€ Tier 1 ($49/mo plan):  $8/mo LLM budget  â†’ ~16% COGS target
â”œâ”€â”€ Tier 2 ($99/mo plan):  $18/mo LLM budget â†’ ~18% COGS target
â””â”€â”€ Tier 3 ($199/mo plan): $35/mo LLM budget â†’ ~18% COGS target

Per-Session Limits:
â”œâ”€â”€ Single conversation: $0.50 hard cap
â”œâ”€â”€ Tokens per minute:   50,000 TPM cap
â””â”€â”€ Max requests/session: 100 LLM calls

Alert Thresholds (per customer):
â”œâ”€â”€ 50% budget consumed â†’ Log warning
â”œâ”€â”€ 80% budget consumed â†’ Slack/email alert to ops
â””â”€â”€ 95% budget consumed â†’ Degrade to cheaper model
â””â”€â”€ 100% budget consumed â†’ Block with user-facing message
```

### Model Routing Rules

| Query Complexity | Target Model | Estimated Cost/Query |
|---|---|---|
| Simple lookups ("what's my spend?") | Gemini Flash / Haiku | $0.001-0.005 |
| Analysis ("why did costs spike?") | Sonnet / Gemini Pro | $0.01-0.05 |
| Deep recommendations ("optimize my Vertex AI") | Sonnet / Opus | $0.05-0.20 |
| Cached/repeated patterns | Local cache (no LLM call) | $0.00 |

### Integration Points

- **Inbound**: MCP Server agent sends all LLM requests to `http://litellm:4000`
- **Outbound**: LiteLLM forwards to Anthropic API, Google Vertex AI, or self-hosted models
- **Logging**: Callbacks configured to send traces to Langfuse via OpenTelemetry
- **Metrics**: Prometheus metrics endpoint exposed at `/metrics` for Grafana scraping

---

## Layer 2: LLM Observability â€” Langfuse

### Role

Langfuse provides **deep LLM-specific observability** â€” prompt/response tracing, cost attribution per customer, session replay, and quality evaluation. While LiteLLM enforces limits, Langfuse tells you *why* costs are what they are and where to optimize.

### Capabilities

| Capability | Purpose | Priority |
|---|---|---|
| **Trace Collection** | Full request lifecycle: prompt â†’ reasoning â†’ tool calls â†’ response | CRITICAL |
| **Cost Attribution** | Per-customer, per-session, per-query cost breakdown | CRITICAL |
| **Session Analysis** | Multi-turn conversation tracking with cost accumulation | HIGH |
| **Model Comparison** | A/B test cost/quality between models for same query types | HIGH |
| **Prompt Management** | Version control and performance tracking of system prompts | MEDIUM |
| **Quality Evaluation** | LLM-as-judge scoring for response quality monitoring | MEDIUM |
| **Token Analytics** | Input vs output token distribution, context window utilization | HIGH |

### Tagging Strategy

Every LLM request is tagged with metadata for granular attribution:

```
Tags per request:
â”œâ”€â”€ customer_id:     "cust_abc123"
â”œâ”€â”€ plan_tier:       "tier_2"
â”œâ”€â”€ session_id:      "sess_xyz789"
â”œâ”€â”€ query_type:      "cost_analysis" | "recommendation" | "simple_lookup" | "action"
â”œâ”€â”€ model_used:      "claude-sonnet-4-20250514"
â”œâ”€â”€ gcp_project:     "customer-project-id"
â”œâ”€â”€ tools_invoked:   ["bigquery_billing", "recommender_api"]
â””â”€â”€ environment:     "production"
```

### Key Dashboards in Langfuse

1. **Cost per Customer** â€” Monthly LLM spend per customer vs. their plan revenue
2. **Cost per Session** â€” Distribution of session costs, identify outlier sessions
3. **Model Usage Mix** â€” Ratio of cheap vs. expensive model usage over time
4. **Token Efficiency** â€” Input/output token ratio, context window utilization
5. **Query Classification** â€” Breakdown of query types and their cost profiles
6. **Margin Tracker** â€” Real-time (Revenue - LLM Cost) per tier

### Architecture Components

```
Langfuse Stack:
â”œâ”€â”€ langfuse-web      â€” Web UI + API server (port 3000)
â”œâ”€â”€ langfuse-worker   â€” Async trace processor
â”œâ”€â”€ postgres           â€” Transactional database (traces metadata, projects, users)
â”œâ”€â”€ clickhouse         â€” OLAP database (trace data, spans, scores â€” optimized for analytics)
â””â”€â”€ redis              â€” Queue + cache (trace ingestion buffering)
```

### Integration Points

- **Inbound**: Receives OTLP traces from LiteLLM proxy callbacks
- **Inbound**: Direct SDK instrumentation from MCP Server for non-LLM spans (tool calls, GCP API latency)
- **Outbound**: Grafana can query Langfuse API for dashboard panels (optional)
- **Storage**: ClickHouse retains trace data for historical cost analysis

---

## Layer 3: Infrastructure Observability â€” Grafana + Prometheus + Loki

### Role

Traditional infrastructure monitoring for the product's runtime environment. Ensures the Cloud Run / Docker services are healthy, performant, and scaling appropriately.

### Components

#### Prometheus â€” Metrics Collection

Scrapes and stores time-series metrics from all product components.

| Metric Source | What It Collects |
|---|---|
| **MCP Server (Cloud Run)** | Request latency, error rates, active sessions, GCP API call duration |
| **LiteLLM Proxy** | LLM request count, token throughput, cache hit rate, model routing decisions, cost per request |
| **Langfuse** | Trace ingestion rate, worker queue depth, database query latency |
| **Postgres** | Connection pool usage, query duration, replication lag |
| **ClickHouse** | Query performance, disk usage, ingestion throughput |
| **Redis** | Memory usage, queue length, eviction rate |
| **Cloud Run** | Instance count, cold start frequency, CPU/memory utilization, billing metrics |

#### Loki + Promtail â€” Log Aggregation

Centralized log collection and search across all services.

| Log Source | Key Signals |
|---|---|
| **MCP Server** | Customer queries, GCP API errors, tool execution logs |
| **LiteLLM** | Rate limit hits, budget enforcement events, model fallbacks, provider errors |
| **Langfuse** | Trace processing errors, ingestion failures |
| **Application** | Authentication events, customer session lifecycle |

#### Grafana â€” Unified Dashboards

Single visualization layer pulling from all data sources.

### Grafana Dashboard Structure

```
Grafana Dashboards:
â”‚
â”œâ”€â”€ ğŸ“Š Executive Overview
â”‚   â”œâ”€â”€ Total LLM spend (today / this week / this month)
â”‚   â”œâ”€â”€ Revenue vs. LLM COGS by tier
â”‚   â”œâ”€â”€ Active customers / sessions
â”‚   â””â”€â”€ System health summary
â”‚
â”œâ”€â”€ ğŸ’° LLM Cost Operations
â”‚   â”œâ”€â”€ Real-time spend rate ($/hour) with anomaly detection
â”‚   â”œâ”€â”€ Per-customer cost breakdown (table + sparklines)
â”‚   â”œâ”€â”€ Model usage distribution (pie: Haiku vs Sonnet vs Opus)
â”‚   â”œâ”€â”€ Cache hit rate and savings
â”‚   â”œâ”€â”€ Budget utilization per customer (gauge charts)
â”‚   â””â”€â”€ Cost outlier sessions (top 10 most expensive)
â”‚
â”œâ”€â”€ ğŸ¤– Agent Performance
â”‚   â”œâ”€â”€ Query latency (p50, p95, p99)
â”‚   â”œâ”€â”€ Tool call success/failure rates
â”‚   â”œâ”€â”€ GCP API response times (BigQuery, Recommender, Asset)
â”‚   â”œâ”€â”€ Conversation length distribution
â”‚   â””â”€â”€ Query type classification breakdown
â”‚
â”œâ”€â”€ ğŸ—ï¸ Infrastructure
â”‚   â”œâ”€â”€ Cloud Run: instances, CPU, memory, cold starts
â”‚   â”œâ”€â”€ Database: Postgres connections, ClickHouse disk, Redis memory
â”‚   â”œâ”€â”€ Network: request throughput, error rates
â”‚   â””â”€â”€ Container health across all services
â”‚
â””â”€â”€ ğŸš¨ Alerts & Incidents
    â”œâ”€â”€ Active alerts timeline
    â”œâ”€â”€ Budget breach events log
    â”œâ”€â”€ Provider outage / degradation history
    â””â”€â”€ Error rate anomalies
```

### Alerting Rules

| Alert | Condition | Severity | Action |
|---|---|---|---|
| **Cost Spike** | LLM spend rate > 3Ã— rolling average | CRITICAL | Slack + PagerDuty |
| **Customer Budget 80%** | Customer monthly LLM spend > 80% of cap | WARNING | Slack notification |
| **Customer Budget 100%** | Customer hits hard cap | INFO | Log + verify degradation active |
| **High Latency** | P95 response time > 10s for 5 min | WARNING | Slack notification |
| **Error Spike** | Error rate > 5% for 3 min | CRITICAL | Slack + PagerDuty |
| **Provider Down** | LLM provider returning 5xx for 2 min | CRITICAL | Verify fallback active |
| **Runaway Session** | Single session > $0.30 | WARNING | Inspect trace in Langfuse |
| **Cold Start Surge** | > 50% requests hitting cold starts | WARNING | Review scaling config |
| **Database Saturation** | Postgres connections > 80% pool | WARNING | Slack notification |
| **Disk Pressure** | ClickHouse disk > 80% | WARNING | Review retention policy |

---

## Data Flow Architecture

### Request Flow (Happy Path)

```
Customer Query
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MCP Server    â”‚  â† Instrumented with Langfuse SDK + Prometheus metrics
â”‚   (Cloud Run)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€ GCP API calls â”€â”€â–º BigQuery, Recommender, Asset Inventory
         â”‚     (latency tracked by Prometheus)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LiteLLM Proxy  â”‚  â† Budget check â†’ Rate limit check â†’ Route to model
â”‚   (AI Gateway)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€ If budget OK â”€â”€â”€â”€â–º LLM Provider (Anthropic/Google/Self-hosted)
         â”‚                              â”‚
         â”‚                              â–¼
         â”‚                        LLM Response
         â”‚                              â”‚
         â”œâ”€â”€â”€â”€ Trace + cost data â”€â”€â–º Langfuse (via OTLP)
         â”œâ”€â”€â”€â”€ Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Prometheus
         â””â”€â”€â”€â”€ Logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Loki
```

### Cost Protection Flow (Enforcement Path)

```
LLM Request arrives at LiteLLM
      â”‚
      â–¼
â”Œâ”€â”€ Budget Check â”€â”€â”
â”‚                  â”‚
â”‚  Within limit?   â”‚
â”‚                  â”‚
â”œâ”€â”€ YES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º Rate Limit Check â”€â”€â”
â”‚                  â”‚                       â”‚
â”œâ”€â”€ > 95% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º Route to CHEAPER   â”‚  Within TPM?
â”‚                  â”‚    model (degrade)    â”‚
â”‚                  â”‚                       â”œâ”€â”€ YES â”€â”€â–º Forward to LLM
â”œâ”€â”€ = 100% â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–º BLOCK request      â”‚
â”‚                  â”‚    Return user msg    â”œâ”€â”€ NO â”€â”€â”€â–º Queue / 429 response
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
                                           â””â”€â”€â–º Log event to Langfuse
                                                Alert via Grafana
```

---

## Deployment Architecture

### Docker Compose Topology

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Docker Compose / Cloud Run                       â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  APPLICATION TIER                                             â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚   â”‚
â”‚  â”‚  â”‚ MCP Server  â”‚â”€â”€â”€â–ºâ”‚ LiteLLM      â”‚â”€â”€â”€â–º LLM Providers      â”‚   â”‚
â”‚  â”‚  â”‚ (Agent)     â”‚    â”‚ Proxy        â”‚    (Anthropic/Google)   â”‚   â”‚
â”‚  â”‚  â”‚ :8080       â”‚    â”‚ :4000        â”‚                         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LLM OBSERVABILITY TIER                                       â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚   â”‚
â”‚  â”‚  â”‚ Langfuse Web â”‚  â”‚ Langfuse    â”‚                           â”‚   â”‚
â”‚  â”‚  â”‚ :3000        â”‚  â”‚ Worker      â”‚                           â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚  â”‚ Postgres â”‚  â”‚ ClickHouse â”‚  â”‚  Redis  â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚ :5432    â”‚  â”‚ :8123      â”‚  â”‚  :6379  â”‚                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  INFRASTRUCTURE OBSERVABILITY TIER                            â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚   â”‚
â”‚  â”‚  â”‚ Prometheus â”‚  â”‚   Loki   â”‚  â”‚ Promtail  â”‚                â”‚   â”‚
â”‚  â”‚  â”‚ :9090      â”‚  â”‚  :3100   â”‚  â”‚ (sidecar) â”‚                â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚   â”‚
â”‚  â”‚                                                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚   â”‚
â”‚  â”‚  â”‚  Grafana   â”‚  â† Unified dashboards (all data sources)    â”‚   â”‚
â”‚  â”‚  â”‚  :3001     â”‚                                              â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Resource Estimates

| Service | RAM | CPU | Disk | Notes |
|---|---|---|---|---|
| MCP Server | 256-512 MB | 0.5 vCPU | â€” | Stateless, scales horizontally |
| LiteLLM Proxy | 512 MB | 0.5 vCPU | â€” | In-memory budget tracking |
| Langfuse Web | 512 MB | 0.5 vCPU | â€” | UI + API |
| Langfuse Worker | 256 MB | 0.25 vCPU | â€” | Async processing |
| Postgres | 512 MB | 0.5 vCPU | 10 GB | Transactional data |
| ClickHouse | 1 GB | 0.5 vCPU | 20 GB | Trace analytics (grows with usage) |
| Redis | 256 MB | 0.25 vCPU | â€” | Queue + cache |
| Prometheus | 512 MB | 0.25 vCPU | 10 GB | 15-day retention default |
| Loki | 512 MB | 0.25 vCPU | 10 GB | Log storage |
| Promtail | 128 MB | 0.1 vCPU | â€” | Log shipping sidecar |
| Grafana | 256 MB | 0.25 vCPU | 1 GB | Dashboards + alerting |
| **TOTAL** | **~4.5-5 GB** | **~3.5 vCPU** | **~51 GB** | Fits on single e2-standard-4 |

### Estimated Monthly Infrastructure Cost (Monitoring Stack Only)

| Deployment Option | Monthly Cost | Notes |
|---|---|---|
| **Single GCE VM (e2-standard-4)** | $95-120 | Simplest, good for early stage |
| **Cloud Run (all services)** | $60-150 | Variable with usage, auto-scaling |
| **GKE Autopilot** | $150-250 | Best for scaling, higher baseline |

---

## Technology Decision Matrix

| Concern | Considered Options | Selected | Rationale |
|---|---|---|---|
| **AI Gateway** | Helicone, Portkey, custom | **LiteLLM** | Open-source, self-hosted, budget enforcement built-in, 100+ provider support |
| **LLM Observability** | Langfuse, Helicone, custom Grafana | **Langfuse** | MIT license, self-hosted, best cost attribution, ClickHouse analytics, native LiteLLM integration |
| **Metrics** | Datadog, Cloud Monitoring, Prometheus | **Prometheus** | Free, proven, Cloud Run native metrics, extensible |
| **Logs** | Cloud Logging, ELK, Loki | **Loki** | Lightweight, Grafana-native, label-based (no full-text indexing overhead) |
| **Visualization** | Datadog, Langfuse UI only, Grafana | **Grafana** | Free, unifies all data sources, extensible, alerting built-in |
| **Traces (infra)** | Jaeger, Tempo, Zipkin | **Grafana Tempo** (optional) | Grafana-native, OTLP-compatible, add later if needed |

---

## Why This Stack (and Not Just Grafana+Prometheus)

The critical distinction is **proactive vs. reactive** cost protection:

| Approach | How It Works | When You Learn About Problems |
|---|---|---|
| Prometheus alert on cost metric | Scrapes metric â†’ evaluates rule â†’ fires alert | **After** the costly request completes |
| LiteLLM budget enforcement | Checks budget **before** forwarding request | **Before** the cost is incurred |

Prometheus alerting tells you "a customer just spent $2 on one session." LiteLLM budget caps prevent that session from exceeding $0.50 in the first place. Both are necessary â€” enforcement for prevention, monitoring for visibility and optimization.

Similarly, Grafana dashboards show aggregate trends, but Langfuse shows you the actual prompt that cost $0.15 and lets you decide whether to optimize it. They serve different audiences: Grafana for ops, Langfuse for product/ML engineering.

---

## Retention & Scaling Considerations

| Data Type | Retention | Scaling Strategy |
|---|---|---|
| Prometheus metrics | 15 days local, optional Thanos for long-term | Increase retention or add remote storage as customer base grows |
| Loki logs | 7 days (configurable) | Compress + ship to GCS for archival |
| Langfuse traces | 90 days in ClickHouse | ClickHouse handles well; archive older traces to GCS |
| LiteLLM budget state | In-memory + Postgres | Stateless restart recovery from DB |

---

## Phase Rollout

| Phase | Components | Timeline | Purpose |
|---|---|---|---|
| **Phase 1** | LiteLLM + Langfuse | Week 1-2 | Core cost protection + LLM visibility |
| **Phase 2** | Prometheus + Grafana | Week 3-4 | Infrastructure monitoring + unified dashboards |
| **Phase 3** | Loki + Promtail | Week 4-5 | Centralized logging |
| **Phase 4** | Advanced Grafana dashboards + alerting | Week 5-6 | Operational maturity |
| **Phase 5** | Tempo (optional) | As needed | Distributed tracing for infrastructure |
